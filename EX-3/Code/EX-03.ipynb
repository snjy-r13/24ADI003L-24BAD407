{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-main",
   "metadata": {},
   "source": [
    "# Multilinear & Polynomial Regression Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def sanjay():\n",
    "    print(\"SANJAY R - 24BAD407\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenario1-header",
   "metadata": {},
   "source": [
    "---\n",
    "# SCENARIO 1: Multilinear Regression on Student Performance\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data-1",
   "metadata": {},
   "source": [
    "### Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"/kaggle/input/datasets/spscientist/students-performance-in-exams/StudentsPerformance.csv\")\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\" \")\n",
    "print(df.head())\n",
    "print(\" \")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create-target",
   "metadata": {},
   "source": [
    "### Create Target Variable\n",
    "Calculate average final score from math, reading, and writing scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "target-var",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"final_score\"] = (df[\"math score\"] + df[\"reading score\"] + df[\"writing score\"]) / 3\n",
    "\n",
    "print(\"Target Variable Statistics:\")\n",
    "print(df[\"final_score\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simulate-features",
   "metadata": {},
   "source": [
    "### Simulate Additional Features\n",
    "Add study hours, attendance, and sleep hours for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sim-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "df[\"study_hours\"] = np.random.randint(1, 8, size=len(df))\n",
    "df[\"attendance\"] = np.random.randint(60, 100, size=len(df))\n",
    "df[\"sleep_hours\"] = np.random.randint(4, 9, size=len(df))\n",
    "\n",
    "print(\"Updated Dataset with Simulated Features:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-selection",
   "metadata": {},
   "source": [
    "### Feature and Target Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xy-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(columns=[\"math score\", \"reading score\", \"writing score\", \"final_score\"])\n",
    "y = df[\"final_score\"]\n",
    "\n",
    "print(\"Features (X):\")\n",
    "print(X.head())\n",
    "print(\"\\nTarget (y):\")\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identify-types",
   "metadata": {},
   "source": [
    "### Identify Feature Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-types",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "numerical_features = X.select_dtypes(exclude=\"object\").columns.tolist()\n",
    "\n",
    "print(\"Categorical Features:\", categorical_features)\n",
    "print(\"Numerical Features:\", numerical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing",
   "metadata": {},
   "source": [
    "### Create Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numerical_features),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Preprocessing pipeline created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-test",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-model",
   "metadata": {},
   "source": [
    "### Train Multilinear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Linear Regression model trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prediction",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Sample Predictions vs Actual:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Actual\": y_test.values[:10],\n",
    "    \"Predicted\": y_pred[:10],\n",
    "    \"Difference\": y_test.values[:10] - y_pred[:10]\n",
    "})\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"LINEAR REGRESSION MODEL PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Mean Squared Error (MSE)  : {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error   : {rmse:.4f}\")\n",
    "print(f\"R² Score                  : {r2:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coefficients",
   "metadata": {},
   "source": [
    "### Analyze Regression Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_names = (\n",
    "    numerical_features +\n",
    "    model.named_steps[\"preprocessing\"]\n",
    "         .transformers_[1][1]\n",
    "         .get_feature_names_out(categorical_features).tolist()\n",
    ")\n",
    "\n",
    "coefficients = model.named_steps[\"regressor\"].coef_\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Coefficient\": coefficients\n",
    "}).sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Feature Coefficients:\")\n",
    "print(coef_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ridge",
   "metadata": {},
   "source": [
    "### Ridge Regression (L2 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ridge-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Pipeline(steps=[\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"regressor\", Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "print(\"Ridge Regression Performance:\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_ridge)):.4f}\")\n",
    "print(f\"R² Score: {r2_score(y_test, y_pred_ridge):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasso",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasso-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Pipeline(steps=[\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"regressor\", Lasso(alpha=0.01, max_iter=10000))\n",
    "])\n",
    "\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "print(\"Lasso Regression Performance:\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_lasso)):.4f}\")\n",
    "print(f\"R² Score: {r2_score(y_test, y_pred_lasso):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz1",
   "metadata": {},
   "source": [
    "#### Predicted vs Actual Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6, edgecolors='k', s=80)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel(\"Actual Final Score\", fontsize=12)\n",
    "plt.ylabel(\"Predicted Final Score\", fontsize=12)\n",
    "plt.title(\"Predicted vs Actual Final Scores\", fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz2",
   "metadata": {},
   "source": [
    "#### Top Feature Influences (Coefficient Magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "top_features = coef_df.set_index(\"Feature\").abs().sort_values(\n",
    "    by=\"Coefficient\", ascending=False\n",
    ").head(10)\n",
    "\n",
    "top_features.plot(kind=\"barh\", legend=False, color='steelblue')\n",
    "plt.xlabel(\"Absolute Coefficient Value\", fontsize=12)\n",
    "plt.ylabel(\"Features\", fontsize=12)\n",
    "plt.title(\"Top 10 Feature Influences on Final Score\", fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz3",
   "metadata": {},
   "source": [
    "#### Residual Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot3",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True, color='coral', bins=30)\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "plt.xlabel(\"Residuals (Actual - Predicted)\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.title(\"Distribution of Residuals\", fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Residual: {residuals.mean():.4f}\")\n",
    "print(f\"Std Residual: {residuals.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenario2-header",
   "metadata": {},
   "source": [
    "---\n",
    "# SCENARIO 2: Polynomial Regression on Auto MPG Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data-2",
   "metadata": {},
   "source": [
    "### Load Auto MPG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-auto",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/datasets/organizations/uciml/autompg-dataset/auto-mpg.csv\")\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-data",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"horsepower\"] = df[\"horsepower\"].replace(\"?\", np.nan)\n",
    "df[\"horsepower\"] = df[\"horsepower\"].astype(float)\n",
    "\n",
    "print(\"Missing values in horsepower:\", df[\"horsepower\"].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handle-missing",
   "metadata": {},
   "source": [
    "### Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fillna",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_hp = df[\"horsepower\"].mean()\n",
    "df[\"horsepower\"].fillna(mean_hp, inplace=True)\n",
    "\n",
    "print(f\"Filled missing values with mean: {mean_hp:.2f}\")\n",
    "print(\"Missing values after filling:\", df[\"horsepower\"].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "select-features-2",
   "metadata": {},
   "source": [
    "### Feature and Target Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xy-auto",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"horsepower\"]]\n",
    "y = df[\"mpg\"]\n",
    "\n",
    "print(\"Feature (X) shape:\", X.shape)\n",
    "print(\"Target (y) shape:\", y.shape)\n",
    "print(\"\\nTarget statistics:\")\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-2",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-auto",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scaling",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "poly-function",
   "metadata": {},
   "source": [
    "### Polynomial Regression Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "poly-def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_model(degree):\n",
    "    \"\"\"\n",
    "    Train a polynomial regression model of given degree\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    degree : int\n",
    "        Degree of polynomial features\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : LinearRegression\n",
    "        Trained model\n",
    "    poly : PolynomialFeatures\n",
    "        Polynomial feature transformer\n",
    "    mse, rmse, r2 : float\n",
    "        Evaluation metrics\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    \n",
    "    X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "    X_test_poly = poly.transform(X_test_scaled)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test_poly)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return model, poly, mse, rmse, r2\n",
    "\n",
    "print(\"Polynomial regression function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-poly",
   "metadata": {},
   "source": [
    "### Train Models with Different Polynomial Degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-degrees",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "\n",
    "print(\"Training polynomial models...\\n\")\n",
    "for d in degrees:\n",
    "    model, poly, mse, rmse, r2 = polynomial_model(d)\n",
    "    results[d] = {\"MSE\": mse, \"RMSE\": rmse, \"R²\": r2}\n",
    "    print(f\"Degree {d}: RMSE={rmse:.4f}, R²={r2:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"POLYNOMIAL REGRESSION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(results_df)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "### 2.9 Model Comparison Table\n",
    "\n",
    "| **Degree** | **Behavior**                    | **Typical Outcome**        |\n",
    "|------------|---------------------------------|----------------------------|\n",
    "| 1          | Linear fit                      | Underfitting               |\n",
    "| 2          | Quadratic curve                 | Good bias-variance balance |\n",
    "| 3          | Cubic curve                     | Better fit                 |\n",
    "| 4          | Higher-order polynomial         | Overfitting risk           |\n",
    "| 5          | Very complex curve              | High overfitting risk      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-poly",
   "metadata": {},
   "source": [
    "### Visualization: Polynomial Regression Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_range = np.linspace(X.min().values[0], X.max().values[0], 300).reshape(-1, 1)\n",
    "X_range_scaled = scaler.transform(X_range)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(X, y, alpha=0.4, s=30, c='gray', label=\"Actual Data\", edgecolors='k', linewidths=0.5)\n",
    "\n",
    "colors = ['blue', 'green', 'orange', 'red', 'purple']\n",
    "for i, d in enumerate([1, 2, 3, 4, 5]):\n",
    "    poly = PolynomialFeatures(d)\n",
    "    X_poly = poly.fit_transform(X_train_scaled)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y_train)\n",
    "    \n",
    "    y_curve = model.predict(poly.transform(X_range_scaled))\n",
    "    plt.plot(X_range, y_curve, label=f\"Degree {d}\", linewidth=2.5, color=colors[i])\n",
    "\n",
    "plt.xlabel(\"Horsepower\", fontsize=12)\n",
    "plt.ylabel(\"MPG (Miles Per Gallon)\", fontsize=12)\n",
    "plt.title(\"Polynomial Regression: Effect of Degree on Model Fit\", fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='upper right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overfitting-proof",
   "metadata": {},
   "source": [
    "### Overfitting vs Underfitting Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overfit-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors = []\n",
    "test_errors = []\n",
    "degrees_extended = list(range(1, 11))\n",
    "\n",
    "for d in degrees_extended:\n",
    "    poly = PolynomialFeatures(d)\n",
    "    X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "    X_test_poly = poly.transform(X_test_scaled)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    train_errors.append(mean_squared_error(y_train, model.predict(X_train_poly)))\n",
    "    test_errors.append(mean_squared_error(y_test, model.predict(X_test_poly)))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(degrees_extended, train_errors, marker='o', markersize=8, \n",
    "         linewidth=2.5, label=\"Training Error\", color='blue', linestyle='-')\n",
    "plt.plot(degrees_extended, test_errors, marker='s', markersize=8, \n",
    "         linewidth=2.5, label=\"Test Error\", color='red', linestyle='-')\n",
    "\n",
    "\n",
    "optimal_degree = degrees_extended[np.argmin(test_errors)]\n",
    "min_test_error = min(test_errors)\n",
    "plt.axvline(x=optimal_degree, color='green', linestyle='--', linewidth=2, alpha=0.7,\n",
    "            label=f'Optimal Degree = {optimal_degree}')\n",
    "\n",
    "\n",
    "plt.axvspan(1, 2, alpha=0.1, color='orange', label='Underfitting Region')\n",
    "plt.axvspan(6, 10, alpha=0.1, color='red', label='Overfitting Region')\n",
    "\n",
    "plt.xlabel(\"Polynomial Degree\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\", fontsize=12, fontweight='bold')\n",
    "plt.title(\"Bias-Variance Tradeoff: Overfitting vs Underfitting\", \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.xticks(degrees_extended)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal Polynomial Degree: {optimal_degree}\")\n",
    "print(f\"Minimum Test Error: {min_test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ridge-poly",
   "metadata": {},
   "source": [
    "### Ridge Regression for Overfitting Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ridge-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "poly_high = PolynomialFeatures(degree=5)\n",
    "\n",
    "X_train_poly = poly_high.fit_transform(X_train_scaled)\n",
    "X_test_poly = poly_high.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "alphas = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RIDGE REGRESSION WITH DEGREE 5 POLYNOMIAL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Alpha':<10} {'RMSE':<15} {'R² Score':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_poly, y_train)\n",
    "    y_pred_ridge = ridge.predict(X_test_poly)\n",
    "    \n",
    "    rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "    r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "    \n",
    "    print(f\"{alpha:<10} {rmse_ridge:<15.4f} {r2_ridge:<15.4f}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "ridge_optimal = Ridge(alpha=1.0)\n",
    "ridge_optimal.fit(X_train_poly, y_train)\n",
    "y_pred_ridge_optimal = ridge_optimal.predict(X_test_poly)\n",
    "\n",
    "print(f\"\\nOptimal Ridge Regression (alpha=1.0):\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_ridge_optimal)):.4f}\")\n",
    "print(f\"R² Score: {r2_score(y_test, y_pred_ridge_optimal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-comparison",
   "metadata": {},
   "source": [
    "### Final Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-comp",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comparison_results = []\n",
    "\n",
    "\n",
    "model_linear, poly_linear, mse_linear, rmse_linear, r2_linear = polynomial_model(1)\n",
    "comparison_results.append(['Linear (Degree 1)', rmse_linear, r2_linear])\n",
    "\n",
    "model_poly3, poly_3, mse_3, rmse_3, r2_3 = polynomial_model(3)\n",
    "comparison_results.append(['Polynomial (Degree 3)', rmse_3, r2_3])\n",
    "\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge_optimal))\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge_optimal)\n",
    "comparison_results.append(['Ridge (Degree 5, α=1.0)', rmse_ridge, r2_ridge])\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results, columns=['Model', 'RMSE', 'R² Score'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_model_idx = comparison_df['RMSE'].idxmin()\n",
    "print(f\"\\nBest Model: {comparison_df.loc[best_model_idx, 'Model']}\")\n",
    "print(f\"Best RMSE: {comparison_df.loc[best_model_idx, 'RMSE']:.4f}\")\n",
    "print(f\"Best R² Score: {comparison_df.loc[best_model_idx, 'R² Score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1489,
     "sourceId": 2704,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 74977,
     "sourceId": 169835,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
